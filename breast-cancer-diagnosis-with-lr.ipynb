{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b0dda6b809c8375b1ef187e65575326c87b164e1"
   },
   "source": [
    "Fetch the dataset into pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6fe9d67a9e5f556d1e79bbef98f4fce8fd096aea"
   },
   "source": [
    "<a id=\"2\"></a> \n",
    "## Data Exploration and Cleaning\n",
    "\n",
    "To have a basic understanding of the data, we have to use pandas' `info` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "7069fba58686045b8de0384562664da0029b05d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      " 32  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "54ebb867e61b3fb22f8b4331b2ca3cf264939fdf"
   },
   "source": [
    "* There are 569 rows and 33 columns in the dataset.\n",
    "* `id` is an attribute with integer values, `diagnosis` is an object (string) type variable and the rest of the features are float numbers.\n",
    "* Luckily, there is no `NaN` values, excepting `Unnamed: 32` feature. All of the entries have `NaN` values for this column. So it has to be removed from the dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "1d7c69546683376b0be339ac7210ba5f71e7a7f9"
   },
   "outputs": [],
   "source": [
    "data.drop(['Unnamed: 32'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "97bad6d4d68ded1660b1ffa641ded09ec6db16fe"
   },
   "source": [
    "Now let's take a glimpse of data with the first five entries, by using `head` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "3329e3e901d95512aecb4eefee02e884da0579ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6bceb435f6829719f4bea4221682c4f44137d05e"
   },
   "source": [
    "As we can observe from the table above, `id` is a unique value per individual patient.  \n",
    "It has nothing to do with diagnosis, so we have to eliminate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "57428ecbb9338983881e541db13b85cb6db67152"
   },
   "outputs": [],
   "source": [
    "data.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "47be60f05385094ee616570eee21eb9a32eac298"
   },
   "source": [
    "<a id=\"3\"></a> \n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "a406c7452e1e9cf218efdd26f8a8f2f5aaff97d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B    357\n",
       "M    212\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aebd041a7a8bbdbf2d92894d39f6081ce38aec2e"
   },
   "source": [
    "`diagnosis` is the label (target) of the dataset.  \n",
    "* `B` indicates Benign  \n",
    "* `M` indicates Malignant\n",
    "\n",
    "When it comes to label, we do not prefer working with object-type variables. So let's change them:\n",
    "* `1` → `M`\n",
    "* `0` → `B`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "50511def4f6910a75ad446a6371f3dd92495ad6c"
   },
   "outputs": [],
   "source": [
    "data.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a5bd0d979df7e235388e0cb9f7fb2b7bfcf749a"
   },
   "source": [
    "Target feature will be held in `y`, and the rest of attributes the will be held in `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "f99dde767d4a863509d5fa9878cd832d998ab554"
   },
   "outputs": [],
   "source": [
    "y = data.diagnosis.values\n",
    "x = data.drop(['diagnosis'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14e206d10209f6004e002586282211407630d7f1"
   },
   "source": [
    "Normalizing variables is importing to avoid imbalanced weights during learning. We want all values to be scaled between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "7d90f30339bf7ec344707dc5e3b827b3f565940f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\lib\\site-packages\\numpy\\core\\fromnumeric.py:84: FutureWarning: In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n",
      "D:\\ANACONDA\\lib\\site-packages\\numpy\\core\\fromnumeric.py:84: FutureWarning: In a future version, DataFrame.max(axis=None) will return a scalar max over the entire DataFrame. To retain the old behavior, use 'frame.max(axis=0)' or just 'frame.max()'\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n",
      "D:\\ANACONDA\\lib\\site-packages\\numpy\\core\\fromnumeric.py:84: FutureWarning: In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "x = (x - np.min(x) ) / ( np.max(x) - np.min(x) ).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "77db9d889270e0e845e2aafe51cc51d8baca896d"
   },
   "source": [
    "<a id=\"4\"></a> \n",
    "## Split Test and Train Variables\n",
    "\n",
    "We have to split our data to create train and test variables. To do so, we will be using `sklearn`'s `train_test_split method`.  \n",
    "\n",
    "We set the`test_size` parameter to `0.2`, so the train values will be randomly 80% of the data.  Let's briefly describe what all four values correspond to:\n",
    "* `x_train` : randomly 80% of data with features of `x` (`radius_mean`, `texture_mean`, etc.)\n",
    "* `x_test` : randomly 20% (the rest) of data with features of `x`\n",
    "* `y_train` : randomly 80% of data with feature of `y` (`diagnosis`, the target feature)\n",
    "* `y_test` : randomly 20% (the rest) of data with feature of `y`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "536aa90caa8c0e7e1194f7d53c7c7c62d3d13814"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 5)\n",
    "\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "bc4b56c3ffc788a797d079a963b70a8bc47ae5f6"
   },
   "outputs": [],
   "source": [
    "def initialize_weights_and_bias(dimension):\n",
    "    w = np.full((dimension,1),0.01)\n",
    "    b = 0.0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "391554ac4e93e0425e8b79622d08c6b212f80a9f"
   },
   "source": [
    "We need the optimized values of bias and weights so that the algorithm will produce the outputs that are closest to real values.\n",
    "\n",
    "\n",
    "\n",
    "After the summation part, we get the output ( $\\large ý$ ) of our algorithm. We need a **threshold** to check whether the numeric output corresponds to `Malignant` or `Benign`. The **sigmoid function** comes handy here to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce8b32a6b4da9e8d45dd2ebf717ac07617f1073d"
   },
   "source": [
    "<a id=\"6\"></a> <br>\n",
    "### Sigmoid Function\n",
    "\n",
    "A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. [Source of definition](https://en.wikipedia.org/wiki/Sigmoid_function)  \n",
    "The formula of the sigmoid function is:\n",
    "\n",
    "$$\\LARGE f(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "The figure below is how it looks like:\n",
    "\n",
    "![sigmoid-function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)\n",
    "\n",
    "We will be using sigmoid function as threshold to determine whether output corresponds to 0 or 1. If value is below the threshold (< 0.5) it is going to be considered as 0, otherwise 1.\n",
    "\n",
    "After checking the output, we rearrange the bias and weights and go back to first step (and keep doing it until their optimized values).  \n",
    "\n",
    "Let's code our sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "25ba9766779900f52f124422c4c7f748bba24ad2"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    output = 1/( 1 + np.exp(-z) )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "127ca885936d2898503ad15fb10acd0a5790d4e4"
   },
   "source": [
    "After defining the sigmoid function, all we have to do is passing our prediction ( $ \\large ý$ ) to sigmoid function, and getting the output as 0 or 1. In our case, as we defined before, `0` means Benign and `1` means Malignant.  \n",
    "\n",
    "Getting the output via sigmoid function is okay, but how can we know how well we predicted it?  \n",
    "The answer is by using **Loss Functions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e2dea7e49e2434ff85b28c5b085b931a6fad60c"
   },
   "source": [
    "<a id=\"7\"><br></a>\n",
    "### Loss Function\n",
    "\n",
    "Loss function is basicly calculating the difference between predicted value and real value of target feature. But here, we will be using a kind of specialized loss function, which is **log loss (cross entry loss)**.  \n",
    "Let's examine the formula of log loss function:  \n",
    "\n",
    "$$\\LARGE -( y log(ý) + (1-y)log(1-ý) ) $$\n",
    "\n",
    "As you can observe from the formula above, if we predict the value as 0, and the real value of label is 0 too (which means we guessed right), then the output of loss function is 0. It is valid for vice versa (both predicted and real values are 1).  \n",
    "Otherwise if we cannot guess it right, then the output of the function would be so high.  \n",
    "\n",
    "As a conclusion, *lower the output of loss function, better we predicted the label*.\n",
    "\n",
    "We measured the error size of one algorithm via loss function.  We have to add all loss function outputs to get the **cost function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "745b38df959c5b0c6f9e070947d0d427fb249db6"
   },
   "outputs": [],
   "source": [
    "def forward_backward_propagation(w,b,x_train,y_train):\n",
    "    y_ = np.dot(w.T,x_train) + b # numeric output of regression algorithm\n",
    "    y_pre = sigmoid(y_) # binary output of sigmoid function\n",
    "    loss = -y_train*np.log(y_pre)-(1-y_train)*np.log(1-y_pre) # output of loss function\n",
    "    cost = (np.sum(loss))/x_train.shape[1] # x_train.shape[1]  is for scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94345a58007f52aa6aa62e58371a99c40bf15b72"
   },
   "source": [
    "<a id=\"8\"><br></a>\n",
    "### Optimizing Algorithm with Gradient Descent\n",
    "\n",
    "We measured how well our algorithms can predict via cost function. Now it is time to optimize algorithm's parameters (weights and bias) so that it can learn from data and make better predictions. It will be handled by **gradient descent** method.  \n",
    "\n",
    "**What is Gradient Descent?**  \n",
    "Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. [Source of definition](https://en.wikipedia.org/wiki/Gradient_descent)  \n",
    "\n",
    "$$\\LARGE w := w - \\alpha \\frac{\\partial J(w)}{\\partial w} $$\n",
    "\n",
    "Here $\\large J(w)$ is the *cost*, we divide its derivative by $\\large w$'s derivative, so we get a numerical result like $\\large 0.02$. And then subtract it from the initial weight, and then update our weight with its new value. \n",
    "\n",
    "P.S: The $\\large \\alpha$ is the *learning rate*, determines how fast we learn. It shouldn't be too big (jumps over the optimal point) or too small (may never converge).\n",
    "\n",
    "Let's take a look at the schema to understand what I mean:\n",
    "\n",
    "![gradiend_descent](https://cdn-images-1.medium.com/max/1600/0*rBQI7uBhBKE8KT-X.png)  \n",
    "\n",
    "As you can see from the image above, we are updating our weight iteratively, until it converges to a local minimum. After convergence, the right part of the formula ( $\\large \\frac{\\partial J(w)}{\\partial w}$) will keep producing zero, which means weight is at the optimal point, and not about to change anymore.  \n",
    "\n",
    "In fact, we will be using bias in the formula too, and it's gonna look like this:\n",
    "\n",
    "$$\\LARGE w := w - \\alpha \\frac{\\partial J(w,b)}{\\partial (w,b)} $$  \n",
    "\n",
    "What we did above is called **backward propagation**. Let's implement it with 'forward propagation' in one function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "ae5768a3897d6b6fab51c69a1ec0dc11599deb44"
   },
   "outputs": [],
   "source": [
    "def forward_backward_propagation(w,b,x_train,y_train):\n",
    "    # forward propagation\n",
    "    y_ = np.dot(w.T,x_train) + b # numeric output of regression algorithm\n",
    "    y_pre = sigmoid(y_) # binary output of sigmoid function\n",
    "    loss = -y_train*np.log(y_pre)-(1-y_train)*np.log(1-y_pre) # output of loss function\n",
    "    cost = (np.sum(loss))/x_train.shape[1] # x_train.shape[1]  is for scaling\n",
    "    # backward propagation\n",
    "    derivative_weight = (np.dot(x_train,((y_pre-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n",
    "    derivative_bias = np.sum(y_pre-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n",
    "    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n",
    "    return cost,gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4baca9891620569689f8a1d05dd395963653a7e2"
   },
   "source": [
    "We may call one forward and one backward propagation as one iteration. It can take our algorithm to somewhere, but obviously it wouldn't be enough. So we have to pick an arbitrary number for iteration ($\\large n$), and update our algorithm $\\large n$ times for learning.  \n",
    "\n",
    "<a id=\"9\"><br></a>\n",
    "### Update Function\n",
    "Let's create a function `update` and use this function to call the methods above to train our algorithm, and give some feedback about costs by printing cost values and visualizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "2257fc2d4e30a4a978fe65996dab1b4e549d24be"
   },
   "outputs": [],
   "source": [
    "def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n",
    "    cost_list = []\n",
    "    cost_list2 = []\n",
    "    index = []\n",
    "    # updating(learning) parameters is number_of_iterarion times\n",
    "    for i in range(number_of_iterarion):\n",
    "        # make forward and backward propagation and find cost and gradients\n",
    "        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n",
    "        cost_list.append(cost)\n",
    "        # lets update\n",
    "        w = w - learning_rate * gradients[\"derivative_weight\"]\n",
    "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "        if i % 10 == 0:\n",
    "            cost_list2.append(cost)\n",
    "            index.append(i)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    # we update(learn) parameters weights and bias\n",
    "    parameters = {\"weight\": w,\"bias\": b}\n",
    "    plt.plot(index,cost_list2)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterarion\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return parameters, gradients, cost_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c19342c4160241c5402239edb40d8b7478b70a9f"
   },
   "source": [
    "<a id=\"10\"><br></a>\n",
    "### Predict Method\n",
    "And it is time to code the `predict` method to create an interface between us and our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "9eeddd44e9370f42274cdf484ecab9a847c83593"
   },
   "outputs": [],
   "source": [
    "def predict(w,b,x_test):\n",
    "    # x_test is a input for forward propagation\n",
    "    z = sigmoid(np.dot(w.T,x_test)+b)\n",
    "    Y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n",
    "    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "95b5ed8cd1b4fc556b8272f246ae793cdcc04580"
   },
   "source": [
    "<a id=\"11\"><br></a>\n",
    "### Coding Logistic Regression\n",
    "And finally let's put all these things together, i.e, code our custom logistic regression function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "c130ca1ad9823f96d603de037033b7bfc783c643",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693953\n",
      "Cost after iteration 10: 0.497200\n",
      "Cost after iteration 20: 0.403702\n",
      "Cost after iteration 30: 0.348852\n",
      "Cost after iteration 40: 0.312591\n",
      "Cost after iteration 50: 0.286617\n",
      "Cost after iteration 60: 0.266939\n",
      "Cost after iteration 70: 0.251403\n",
      "Cost after iteration 80: 0.238747\n",
      "Cost after iteration 90: 0.228182\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 94.72527472527473 %\n",
      "test accuracy: 93.85964912280701 %\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n",
    "    \n",
    "    dimension =  x_train.shape[0]\n",
    "    w,b = initialize_weights_and_bias(dimension)\n",
    "    \n",
    "    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n",
    "    \n",
    "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "    \n",
    "logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bccd3d455917603e9e8914d9db9ac5b735252c3c"
   },
   "source": [
    "<a id=\"12\"><br></a>\n",
    "## Conclusions\n",
    "* Logistic Regression is a good way to classify binary labels, concluded from a lot of numerical features.\n",
    "* Cost gets smaller as the iteration of update (forward and backward propagation) increases.\n",
    "* We got an accuracy of `~93.86` on the `test` dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
